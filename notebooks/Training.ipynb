{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SiTIpPjArIyr"
   },
   "source": [
    "# Full example with the Hugging Face Transformers package\n",
    "\n",
    "This notebook shows how to train a model (Mistral) and generate music with it, using the Hugging Face Transformers package."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gOd93yV0sGd2"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "fX12Yquyuihc"
   },
   "outputs": [],
   "source": [
    "#@title Install all dependencies (run only once per session)\n",
    "\n",
    "# !nvidia-smi\n",
    "\n",
    "# !pip install miditok\n",
    "# !pip install symusic\n",
    "# !pip install torch\n",
    "# !pip install torchtoolkit\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install evaluate\n",
    "# !pip install tqdm\n",
    "# !pip install scikit-learn\n",
    "# !pip install tensorboard\n",
    "\n",
    "# !wget https://storage.googleapis.com/magentadata/datasets/maestro/v3.0.0/maestro-v3.0.0-midi.zip\n",
    "# !unzip 'maestro-v3.0.0-midi.zip'\n",
    "# !rm 'maestro-v3.0.0-midi.zip'\n",
    "# !mv 'maestro-v3.0.0' 'Maestro'\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "from torch import Tensor, argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda import is_available as cuda_available, is_bf16_supported\n",
    "from torch.backends.mps import is_available as mps_available\n",
    "from transformers import AutoModelForCausalLM, MistralConfig, Trainer, TrainingArguments, GenerationConfig, MambaConfig, MambaForCausalLM\n",
    "from transformers.trainer_utils import set_seed\n",
    "from evaluate import load as load_metric\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetTok, DataCollator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer initialization and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: ../data/maestro-v3.0.0/2004: 100%|██████████| 893/893 [00:45<00:00, 19.69it/s]\n",
      "Loading data: ../data/maestro-v3.0.0/2015: 100%|██████████| 255/255 [00:13<00:00, 19.06it/s]\n",
      "Loading data: ../data/maestro-v3.0.0/2008: 100%|██████████| 128/128 [00:07<00:00, 17.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Seed\n",
    "set_seed(777)\n",
    "\n",
    "# Our tokenizer's configuration\n",
    "PITCH_RANGE = (21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NUM_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = False\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = False\n",
    "NUM_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"num_velocities\": NUM_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"num_tempos\": NUM_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer\n",
    "# tokenizer = REMI(config)\n",
    "tokenizer = REMI(params='./tokenizer.json')\n",
    "\n",
    "# Trains the tokenizer with Byte Pair Encoding (BPE) to build the vocabulary, here 10k tokens\n",
    "midi_paths = list(Path('../data/maestro-v3.0.0').glob('**/*.mid')) + list(Path('../data/maestro-v3.0.0').glob('**/*.midi'))\n",
    "# tokenizer.learn_bpe(\n",
    "#     vocab_size=10000,\n",
    "#     files_paths=midi_paths,\n",
    "#     start_from_empty_voc=False,\n",
    "# )\n",
    "# tokenizer.save_params(\"tokenizer.json\")\n",
    "\n",
    "# Split MIDI paths in train/valid/test sets\n",
    "total_num_files = len(midi_paths)\n",
    "num_files_valid = round(total_num_files * 0.2)\n",
    "num_files_test = round(total_num_files * 0.1)\n",
    "shuffle(midi_paths)\n",
    "midi_paths_valid = midi_paths[:num_files_valid]\n",
    "midi_paths_test = midi_paths[num_files_valid:num_files_valid + num_files_test]\n",
    "midi_paths_train = midi_paths[num_files_valid + num_files_test:]\n",
    "\n",
    "# Loads tokens and create data collator\n",
    "kwargs_dataset = {\"min_seq_len\": 256, \"max_seq_len\": 1024, \"tokenizer\": tokenizer}\n",
    "dataset_train = DatasetTok(midi_paths_train, **kwargs_dataset)\n",
    "dataset_valid = DatasetTok(midi_paths_valid, **kwargs_dataset)\n",
    "dataset_test = DatasetTok(midi_paths_test, **kwargs_dataset)\n",
    "collator = DataCollator(\n",
    "    tokenizer[\"PAD_None\"], tokenizer[\"BOS_None\"], tokenizer[\"EOS_None\"]\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization\n",
    "\n",
    "We will use the [Mistral implementation of Hugging Face](https://huggingface.co/docs/transformers/model_doc/mistral).\n",
    "Feel free to explore the documentation and source code to dig deeper.\n",
    "\n",
    "**You may need to adjust the model's configuration, the training configuration and the maximum input sequence length (cell above) depending on your hardware.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "model_config = MistralConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=512,\n",
    "    intermediate_size=2048,\n",
    "    num_hidden_layers=8,\n",
    "    num_attention_heads=8,\n",
    "    num_key_value_heads=4,\n",
    "    sliding_window=256,\n",
    "    max_position_embeddings=8192,\n",
    "    pad_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None'],\n",
    ")\n",
    "model = AutoModelForCausalLM.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = MambaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=192,\n",
    "    state_size=8,\n",
    "    max_position_embeddings=8192,\n",
    "    num_hidden_layers=16,\n",
    "    pad_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None'],\n",
    ")\n",
    "\n",
    "model = MambaForCausalLM(model_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800128"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 16\n",
      "***** Running training *****\n",
      "  Num examples = 7,658\n",
      "  Num Epochs = 13\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 2,000\n",
      "  Number of trainable parameters = 5,800,128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b70f0d2a5eb400c8af8b766fa60b509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filif/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.8543, 'grad_norm': 1.1173369884490967, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.13}\n",
      "{'loss': 10.7975, 'grad_norm': 1.087349772453308, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.25}\n",
      "{'loss': 10.6804, 'grad_norm': 1.0429495573043823, 'learning_rate': 1e-05, 'epoch': 0.38}\n",
      "{'loss': 10.5176, 'grad_norm': 1.0284433364868164, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.5}\n",
      "{'loss': 10.3007, 'grad_norm': 1.1212576627731323, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.63}\n",
      "{'loss': 9.9798, 'grad_norm': 1.0334638357162476, 'learning_rate': 2e-05, 'epoch': 0.75}\n",
      "{'loss': 9.6176, 'grad_norm': 0.8386845588684082, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.88}\n",
      "{'loss': 9.339, 'grad_norm': 0.6770079731941223, 'learning_rate': 2.6666666666666667e-05, 'epoch': 1.0}\n",
      "{'loss': 9.1099, 'grad_norm': 0.5555304884910583, 'learning_rate': 3e-05, 'epoch': 1.13}\n",
      "{'loss': 8.9373, 'grad_norm': 0.4903627634048462, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.25}\n",
      "{'loss': 8.7797, 'grad_norm': 0.4604906141757965, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.38}\n",
      "{'loss': 8.6362, 'grad_norm': 0.4080934226512909, 'learning_rate': 4e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bae942b193c4f3caeb56f2ffc462dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.493900299072266, 'eval_runtime': 9.1385, 'eval_samples_per_second': 245.553, 'eval_steps_per_second': 5.143, 'epoch': 1.57}\n",
      "{'loss': 8.4852, 'grad_norm': 0.3856232762336731, 'learning_rate': 4.3333333333333334e-05, 'epoch': 1.63}\n",
      "{'loss': 8.396, 'grad_norm': 0.36467087268829346, 'learning_rate': 4.666666666666667e-05, 'epoch': 1.75}\n",
      "{'loss': 8.2906, 'grad_norm': 0.31746095418930054, 'learning_rate': 5e-05, 'epoch': 1.88}\n",
      "{'loss': 8.1911, 'grad_norm': 0.3036254048347473, 'learning_rate': 5.333333333333333e-05, 'epoch': 2.0}\n",
      "{'loss': 8.1044, 'grad_norm': 0.266647070646286, 'learning_rate': 5.666666666666667e-05, 'epoch': 2.13}\n",
      "{'loss': 8.0477, 'grad_norm': 0.237630695104599, 'learning_rate': 6e-05, 'epoch': 2.25}\n",
      "{'loss': 8.0059, 'grad_norm': 0.2264871895313263, 'learning_rate': 6.333333333333333e-05, 'epoch': 2.38}\n",
      "{'loss': 7.9548, 'grad_norm': 0.19846822321414948, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.51}\n",
      "{'loss': 7.9146, 'grad_norm': 0.18411993980407715, 'learning_rate': 7e-05, 'epoch': 2.63}\n",
      "{'loss': 7.868, 'grad_norm': 0.19895124435424805, 'learning_rate': 7.333333333333333e-05, 'epoch': 2.76}\n",
      "{'loss': 7.8588, 'grad_norm': 0.2225957065820694, 'learning_rate': 7.666666666666667e-05, 'epoch': 2.88}\n",
      "{'loss': 7.8262, 'grad_norm': 0.23687098920345306, 'learning_rate': 8e-05, 'epoch': 3.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.7709, 'grad_norm': 0.281116783618927, 'learning_rate': 8.333333333333334e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00b81c0d0a24643bc77d69daeac37df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.728194236755371, 'eval_runtime': 9.062, 'eval_samples_per_second': 247.628, 'eval_steps_per_second': 5.187, 'epoch': 3.13}\n",
      "{'loss': 7.7053, 'grad_norm': 0.41264280676841736, 'learning_rate': 8.666666666666667e-05, 'epoch': 3.26}\n",
      "{'loss': 7.6575, 'grad_norm': 0.4622027277946472, 'learning_rate': 9e-05, 'epoch': 3.38}\n",
      "{'loss': 7.5817, 'grad_norm': 0.4035986661911011, 'learning_rate': 9.333333333333334e-05, 'epoch': 3.51}\n",
      "{'loss': 7.5178, 'grad_norm': 0.34891533851623535, 'learning_rate': 9.666666666666667e-05, 'epoch': 3.63}\n",
      "{'loss': 7.4341, 'grad_norm': 0.6543633937835693, 'learning_rate': 0.0001, 'epoch': 3.76}\n",
      "{'loss': 7.4323, 'grad_norm': 0.49501627683639526, 'learning_rate': 9.994965332706573e-05, 'epoch': 3.88}\n",
      "{'loss': 7.3983, 'grad_norm': 0.4102728068828583, 'learning_rate': 9.979871469976196e-05, 'epoch': 4.01}\n",
      "{'loss': 7.3499, 'grad_norm': 0.7887152433395386, 'learning_rate': 9.954748808839674e-05, 'epoch': 4.13}\n",
      "{'loss': 7.2932, 'grad_norm': 0.5180826783180237, 'learning_rate': 9.919647942993148e-05, 'epoch': 4.26}\n",
      "{'loss': 7.2702, 'grad_norm': 0.9895877838134766, 'learning_rate': 9.874639560909117e-05, 'epoch': 4.38}\n",
      "{'loss': 7.2116, 'grad_norm': 0.5925262570381165, 'learning_rate': 9.819814303479267e-05, 'epoch': 4.51}\n",
      "{'loss': 7.1857, 'grad_norm': 1.339883804321289, 'learning_rate': 9.755282581475769e-05, 'epoch': 4.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6a75d46c9d472d8a97985928f29f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.122819423675537, 'eval_runtime': 9.2801, 'eval_samples_per_second': 241.808, 'eval_steps_per_second': 5.065, 'epoch': 4.7}\n",
      "{'loss': 7.132, 'grad_norm': 0.8340454697608948, 'learning_rate': 9.681174353198687e-05, 'epoch': 4.76}\n",
      "{'loss': 7.091, 'grad_norm': 0.5852659344673157, 'learning_rate': 9.597638862757255e-05, 'epoch': 4.89}\n",
      "{'loss': 7.0676, 'grad_norm': 0.361488401889801, 'learning_rate': 9.504844339512095e-05, 'epoch': 5.01}\n",
      "{'loss': 7.0335, 'grad_norm': 0.46568670868873596, 'learning_rate': 9.40297765928369e-05, 'epoch': 5.14}\n",
      "{'loss': 6.9799, 'grad_norm': 0.5836212635040283, 'learning_rate': 9.292243968009331e-05, 'epoch': 5.26}\n",
      "{'loss': 6.9266, 'grad_norm': 0.6494702100753784, 'learning_rate': 9.172866268606513e-05, 'epoch': 5.39}\n",
      "{'loss': 6.9011, 'grad_norm': 0.7492420077323914, 'learning_rate': 9.045084971874738e-05, 'epoch': 5.51}\n",
      "{'loss': 6.8344, 'grad_norm': 0.4563840329647064, 'learning_rate': 8.90915741234015e-05, 'epoch': 5.64}\n",
      "{'loss': 6.8082, 'grad_norm': 0.7979830503463745, 'learning_rate': 8.765357330018056e-05, 'epoch': 5.76}\n",
      "{'loss': 6.7753, 'grad_norm': 0.8715785145759583, 'learning_rate': 8.613974319136958e-05, 'epoch': 5.89}\n",
      "{'loss': 6.71, 'grad_norm': 0.8250414729118347, 'learning_rate': 8.455313244934324e-05, 'epoch': 6.01}\n",
      "{'loss': 6.7035, 'grad_norm': 0.8353439569473267, 'learning_rate': 8.289693629698564e-05, 'epoch': 6.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6563, 'grad_norm': 0.7212328910827637, 'learning_rate': 8.117449009293668e-05, 'epoch': 6.26}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95c65f5b30c403482c5978fb18d6707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-1000\n",
      "Configuration saved in runs/checkpoint-1000/config.json\n",
      "Configuration saved in runs/checkpoint-1000/generation_config.json\n",
      "Model weights saved in runs/checkpoint-1000/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.6417436599731445, 'eval_runtime': 9.2805, 'eval_samples_per_second': 241.797, 'eval_steps_per_second': 5.064, 'epoch': 6.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filif/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6352, 'grad_norm': 0.7302793264389038, 'learning_rate': 7.938926261462366e-05, 'epoch': 6.39}\n",
      "{'loss': 6.6108, 'grad_norm': 0.49132946133613586, 'learning_rate': 7.754484907260513e-05, 'epoch': 6.51}\n",
      "{'loss': 6.5829, 'grad_norm': 0.7991334795951843, 'learning_rate': 7.564496387029532e-05, 'epoch': 6.64}\n",
      "{'loss': 6.5513, 'grad_norm': 0.6123929619789124, 'learning_rate': 7.369343312364993e-05, 'epoch': 6.76}\n",
      "{'loss': 6.5168, 'grad_norm': 0.8323113918304443, 'learning_rate': 7.169418695587791e-05, 'epoch': 6.89}\n",
      "{'loss': 6.4985, 'grad_norm': 0.6493431329727173, 'learning_rate': 6.965125158269619e-05, 'epoch': 7.01}\n",
      "{'loss': 6.475, 'grad_norm': 0.947213888168335, 'learning_rate': 6.756874120406714e-05, 'epoch': 7.14}\n",
      "{'loss': 6.4686, 'grad_norm': 0.7997612953186035, 'learning_rate': 6.545084971874738e-05, 'epoch': 7.27}\n",
      "{'loss': 6.4473, 'grad_norm': 1.0834264755249023, 'learning_rate': 6.330184227833376e-05, 'epoch': 7.39}\n",
      "{'loss': 6.434, 'grad_norm': 0.9170778393745422, 'learning_rate': 6.112604669781572e-05, 'epoch': 7.52}\n",
      "{'loss': 6.4196, 'grad_norm': 0.4189334511756897, 'learning_rate': 5.8927844739931834e-05, 'epoch': 7.64}\n",
      "{'loss': 6.3875, 'grad_norm': 0.7837825417518616, 'learning_rate': 5.6711663290882776e-05, 'epoch': 7.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e48cbd90d04d168072f4833c3b3ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.379364490509033, 'eval_runtime': 9.1896, 'eval_samples_per_second': 244.19, 'eval_steps_per_second': 5.114, 'epoch': 7.83}\n",
      "{'loss': 6.3945, 'grad_norm': 0.5491718649864197, 'learning_rate': 5.448196544517168e-05, 'epoch': 7.89}\n",
      "{'loss': 6.368, 'grad_norm': 0.4666157066822052, 'learning_rate': 5.2243241517525754e-05, 'epoch': 8.02}\n",
      "{'loss': 6.3646, 'grad_norm': 0.5591413378715515, 'learning_rate': 5e-05, 'epoch': 8.14}\n",
      "{'loss': 6.3454, 'grad_norm': 0.5666479468345642, 'learning_rate': 4.775675848247427e-05, 'epoch': 8.27}\n",
      "{'loss': 6.3303, 'grad_norm': 0.5538848638534546, 'learning_rate': 4.551803455482833e-05, 'epoch': 8.39}\n",
      "{'loss': 6.3255, 'grad_norm': 0.5233960151672363, 'learning_rate': 4.328833670911724e-05, 'epoch': 8.52}\n",
      "{'loss': 6.3087, 'grad_norm': 0.6081569194793701, 'learning_rate': 4.107215526006817e-05, 'epoch': 8.64}\n",
      "{'loss': 6.3241, 'grad_norm': 0.37131088972091675, 'learning_rate': 3.887395330218429e-05, 'epoch': 8.77}\n",
      "{'loss': 6.3025, 'grad_norm': 0.5819216966629028, 'learning_rate': 3.6698157721666246e-05, 'epoch': 8.89}\n",
      "{'loss': 6.2705, 'grad_norm': 0.5700587630271912, 'learning_rate': 3.4549150281252636e-05, 'epoch': 9.02}\n",
      "{'loss': 6.2689, 'grad_norm': 0.8279180526733398, 'learning_rate': 3.243125879593286e-05, 'epoch': 9.14}\n",
      "{'loss': 6.2721, 'grad_norm': 0.5597344040870667, 'learning_rate': 3.0348748417303823e-05, 'epoch': 9.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2747, 'grad_norm': 0.47911709547042847, 'learning_rate': 2.8305813044122097e-05, 'epoch': 9.39}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17f1de9210742d2942b6109f1bc8570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.2642974853515625, 'eval_runtime': 9.2038, 'eval_samples_per_second': 243.812, 'eval_steps_per_second': 5.107, 'epoch': 9.39}\n",
      "{'loss': 6.2627, 'grad_norm': 0.6638137698173523, 'learning_rate': 2.630656687635007e-05, 'epoch': 9.52}\n",
      "{'loss': 6.2618, 'grad_norm': 0.6825015544891357, 'learning_rate': 2.43550361297047e-05, 'epoch': 9.65}\n",
      "{'loss': 6.2671, 'grad_norm': 0.4511934816837311, 'learning_rate': 2.245515092739488e-05, 'epoch': 9.77}\n",
      "{'loss': 6.2483, 'grad_norm': 0.48987722396850586, 'learning_rate': 2.061073738537635e-05, 'epoch': 9.9}\n",
      "{'loss': 6.2506, 'grad_norm': 0.48589959740638733, 'learning_rate': 1.8825509907063327e-05, 'epoch': 10.02}\n",
      "{'loss': 6.2253, 'grad_norm': 0.3956669569015503, 'learning_rate': 1.7103063703014372e-05, 'epoch': 10.15}\n",
      "{'loss': 6.2215, 'grad_norm': 0.49434563517570496, 'learning_rate': 1.544686755065677e-05, 'epoch': 10.27}\n",
      "{'loss': 6.2498, 'grad_norm': 0.3422397971153259, 'learning_rate': 1.3860256808630428e-05, 'epoch': 10.4}\n",
      "{'loss': 6.242, 'grad_norm': 0.5485091209411621, 'learning_rate': 1.2346426699819458e-05, 'epoch': 10.52}\n",
      "{'loss': 6.2352, 'grad_norm': 0.40130123496055603, 'learning_rate': 1.090842587659851e-05, 'epoch': 10.65}\n",
      "{'loss': 6.234, 'grad_norm': 0.47581371665000916, 'learning_rate': 9.549150281252633e-06, 'epoch': 10.77}\n",
      "{'loss': 6.2327, 'grad_norm': 0.5827024579048157, 'learning_rate': 8.271337313934869e-06, 'epoch': 10.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31856bbff8248d593ce811934a1c8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.223368167877197, 'eval_runtime': 9.1226, 'eval_samples_per_second': 245.981, 'eval_steps_per_second': 5.152, 'epoch': 10.96}\n",
      "{'loss': 6.2283, 'grad_norm': 0.420021116733551, 'learning_rate': 7.077560319906695e-06, 'epoch': 11.02}\n",
      "{'loss': 6.2214, 'grad_norm': 0.369074285030365, 'learning_rate': 5.9702234071631e-06, 'epoch': 11.15}\n",
      "{'loss': 6.2199, 'grad_norm': 0.3708091378211975, 'learning_rate': 4.951556604879048e-06, 'epoch': 11.27}\n",
      "{'loss': 6.2219, 'grad_norm': 0.39810821413993835, 'learning_rate': 4.023611372427471e-06, 'epoch': 11.4}\n",
      "{'loss': 6.221, 'grad_norm': 0.3980230391025543, 'learning_rate': 3.18825646801314e-06, 'epoch': 11.52}\n",
      "{'loss': 6.2061, 'grad_norm': 0.3656119108200073, 'learning_rate': 2.4471741852423237e-06, 'epoch': 11.65}\n",
      "{'loss': 6.2133, 'grad_norm': 0.31287604570388794, 'learning_rate': 1.8018569652073381e-06, 'epoch': 11.77}\n",
      "{'loss': 6.223, 'grad_norm': 0.4212947189807892, 'learning_rate': 1.2536043909088191e-06, 'epoch': 11.9}\n",
      "{'loss': 6.2281, 'grad_norm': 0.40359410643577576, 'learning_rate': 8.035205700685167e-07, 'epoch': 12.03}\n",
      "{'loss': 6.2084, 'grad_norm': 0.38332629203796387, 'learning_rate': 4.52511911603265e-07, 'epoch': 12.15}\n",
      "{'loss': 6.224, 'grad_norm': 0.357113778591156, 'learning_rate': 2.012853002380466e-07, 'epoch': 12.28}\n",
      "{'loss': 6.2204, 'grad_norm': 0.3530736565589905, 'learning_rate': 5.0346672934270534e-08, 'epoch': 12.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2244\n",
      "  Batch size = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2218, 'grad_norm': 0.41447681188583374, 'learning_rate': 0.0, 'epoch': 12.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426426ac2c8c4afdb2b5c75ab53f583c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-2000\n",
      "Configuration saved in runs/checkpoint-2000/config.json\n",
      "Configuration saved in runs/checkpoint-2000/generation_config.json\n",
      "Model weights saved in runs/checkpoint-2000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from runs/checkpoint-2000 (score: 6.217372417449951).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
      "Saving model checkpoint to runs\n",
      "Configuration saved in runs/config.json\n",
      "Configuration saved in runs/generation_config.json\n",
      "Model weights saved in runs/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.217372417449951, 'eval_runtime': 9.069, 'eval_samples_per_second': 247.436, 'eval_steps_per_second': 5.182, 'epoch': 12.53}\n",
      "{'train_runtime': 1813.704, 'train_samples_per_second': 52.93, 'train_steps_per_second': 1.103, 'train_loss': 7.168868602752686, 'epoch': 12.53}\n",
      "***** train metrics *****\n",
      "  epoch                    =      12.53\n",
      "  train_loss               =     7.1689\n",
      "  train_runtime            = 0:30:13.70\n",
      "  train_samples_per_second =      52.93\n",
      "  train_steps_per_second   =      1.103\n"
     ]
    }
   ],
   "source": [
    "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess the logits before accumulating them during evaluation.\n",
    "\n",
    "    This allows to significantly reduce the memory usage and make the training tractable.\n",
    "    \"\"\"\n",
    "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
    "    return pred_ids\n",
    "\n",
    "# Create config for the Trainer\n",
    "USE_CUDA = cuda_available()\n",
    "if not cuda_available():\n",
    "    FP16 = FP16_EVAL = BF16 = BF16_EVAL = False\n",
    "elif is_bf16_supported():\n",
    "    BF16 = BF16_EVAL = True\n",
    "    FP16 = FP16_EVAL = False\n",
    "else:\n",
    "    BF16 = BF16_EVAL = False\n",
    "    FP16 = FP16_EVAL = True\n",
    "USE_MPS = not USE_CUDA and mps_available()\n",
    "training_config = TrainingArguments(\n",
    "    \"runs\", False, True, False, False, \"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=48,\n",
    "    gradient_accumulation_steps=3,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_steps=250,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=3.0,\n",
    "    max_steps=2000,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.3,\n",
    "    log_level=\"debug\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5,\n",
    "    no_cuda=not USE_CUDA,\n",
    "    seed=444,\n",
    "    fp16=FP16,\n",
    "    fp16_full_eval=FP16_EVAL,\n",
    "    bf16=BF16,\n",
    "    bf16_full_eval=BF16_EVAL,\n",
    "    load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "collator = DataCollator(tokenizer[\"PAD_None\"], tokenizer[\"BOS_None\"], tokenizer[\"EOS_None\"], copy_inputs_as_labels=True)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_valid,\n",
    "    callbacks=None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits,\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "OaNkGcFo9UP_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./runs/checkpoint-2000/config.json\n",
      "Model config MambaConfig {\n",
      "  \"architectures\": [\n",
      "    \"MambaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"conv_kernel\": 4,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"expand\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"mamba\",\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rescale_prenorm_residual\": false,\n",
      "  \"residual_in_fp32\": true,\n",
      "  \"state_size\": 8,\n",
      "  \"time_step_floor\": 0.0001,\n",
      "  \"time_step_init_scheme\": \"random\",\n",
      "  \"time_step_max\": 0.1,\n",
      "  \"time_step_min\": 0.001,\n",
      "  \"time_step_rank\": 12,\n",
      "  \"time_step_scale\": 1.0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_bias\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_conv_bias\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n",
      "loading weights file ./runs/checkpoint-2000/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MambaForCausalLM.\n",
      "\n",
      "All the weights of MambaForCausalLM were initialized from the model checkpoint at ./runs/checkpoint-2000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.\n",
      "loading configuration file ./runs/checkpoint-2000/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Testing model / Generating results:   0%|          | 0/74 [00:15<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "(gen_results_path := Path('gen_res')).mkdir(parents=True, exist_ok=True)\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=512,  # extends samples by 512 tokens\n",
    "    num_beams=1,        # no beam search\n",
    "    do_sample=True,     # but sample instead\n",
    "    temperature=0.9,\n",
    "    top_k=15,\n",
    "    top_p=0.95,\n",
    "    epsilon_cutoff=3e-4,\n",
    "    eta_cutoff=1e-3,\n",
    "    # pad_token_id=config.padding_token_id,\n",
    ")\n",
    "\n",
    "# Here the sequences are padded to the left, so that the last token along the time dimension\n",
    "# is always the last token of each seq, allowing to efficiently generate by batch\n",
    "collator.pad_on_left = True\n",
    "collator.eos_token = None\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16, collate_fn=collator)\n",
    "\n",
    "model = MambaForCausalLM.from_pretrained('./runs/checkpoint-2000')\n",
    "\n",
    "model.eval()\n",
    "count = 0\n",
    "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):  # (N,T)\n",
    "    res = model.generate(\n",
    "        inputs=batch[\"input_ids\"].to(model.device),\n",
    "        attention_mask=batch[\"attention_mask\"].to(model.device),\n",
    "        generation_config=generation_config)  # (N,T)\n",
    "\n",
    "    # Saves the generated music, as MIDI files and tokens (json)\n",
    "    for prompt, continuation in zip(batch[\"input_ids\"], res):\n",
    "        generated = continuation[len(prompt):]\n",
    "        midi = tokenizer.tokens_to_midi([deepcopy(generated.tolist())])\n",
    "        tokens = [generated, prompt, continuation]  # list compr. as seqs of dif. lengths\n",
    "        tokens = [seq.tolist() for seq in tokens]\n",
    "        # for tok_seq in tokens[1:]:\n",
    "        #     _midi = tokenizer.tokens_to_midi([deepcopy(tok_seq)])\n",
    "        #     midi.instruments.append(_midi.instruments[0])\n",
    "        # midi.instruments[0].name = f'Continuation of original sample ({len(generated)} tokens)'\n",
    "        # midi.instruments[1].name = f'Original sample ({len(prompt)} tokens)'\n",
    "        # midi.instruments[2].name = f'Original sample and continuation'\n",
    "        midi.dump_midi(gen_results_path / f'{count}.mid')\n",
    "        tokenizer.save_tokens(tokens, gen_results_path / f'{count}.json') \n",
    "\n",
    "        count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Testing model / Generating results:   0%|          | 0/74 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected conv_state.scalar_type() == input_type to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader_test, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTesting model / Generating results\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      8\u001b[0m res\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/generation/utils.py:1527\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1510\u001b[0m         input_ids,\n\u001b[1;32m   1511\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/generation/utils.py:2411\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2408\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2411\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2414\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2415\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2419\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:653\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    651\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 653\u001b[0m mamba_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m mamba_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    663\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:555\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(mixer_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cache_params)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmixer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    558\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:327\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_in_fp32:\n\u001b[1;32m    325\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:291\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, cache_params: Optional[MambaCache] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fast_path_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype:\n\u001b[0;32m--> 291\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda_kernels_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_forward(hidden_states, cache_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/transformers/models/mamba/modeling_mamba.py:160\u001b[0m, in \u001b[0;36mMambaMixer.cuda_kernels_forward\u001b[0;34m(self, hidden_states, cache_params)\u001b[0m\n\u001b[1;32m    158\u001b[0m conv_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cache_params\u001b[38;5;241m.\u001b[39mseqlen_offset \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mcausal_conv1d_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pixgan/lib/python3.11/site-packages/causal_conv1d/causal_conv1d_interface.py:187\u001b[0m, in \u001b[0;36mcausal_conv1d_update\u001b[0;34m(x, conv_state, weight, bias, activation)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation must be None, silu, or swish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m activation \u001b[38;5;241m=\u001b[39m activation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswish\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcausal_conv1d_cuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_conv1d_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected conv_state.scalar_type() == input_type to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
     ]
    }
   ],
   "source": [
    "dataloader_test = DataLoader(dataset_test, batch_size=16, collate_fn=collator)\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader_test, desc='Testing model / Generating results'):\n",
    "    res = model.generate(batch['input_ids'].to(model.device), max_new_tokens=100)\n",
    "    break\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Optimus_VIRTUOSO_Multi_Instrumental_RGA_Edition.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
