{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "\n",
    "from torch import Tensor, argmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda import is_available as cuda_available, is_bf16_supported\n",
    "from torch.backends.mps import is_available as mps_available\n",
    "from transformers import AutoModelForCausalLM, MistralConfig, Trainer, TrainingArguments, GenerationConfig, MambaConfig, MambaForCausalLM\n",
    "from transformers.trainer_utils import set_seed\n",
    "from evaluate import load as load_metric\n",
    "from miditok import REMI, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetTok, DataCollator\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: ../data/maestro-v3.0.0/2008: 100%|██████████| 893/893 [00:45<00:00, 19.81it/s]\n",
      "Loading data: ../data/maestro-v3.0.0/2018: 100%|██████████| 255/255 [00:13<00:00, 19.58it/s]\n",
      "Loading data: ../data/maestro-v3.0.0/2015: 100%|██████████| 128/128 [00:05<00:00, 21.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Our tokenizer's configuration\n",
    "PITCH_RANGE = (21, 109)\n",
    "BEAT_RES = {(0, 1): 8, (1, 2): 4, (2, 4): 2, (4, 8): 1}\n",
    "NUM_VELOCITIES = 24\n",
    "SPECIAL_TOKENS = [\"PAD\", \"MASK\", \"BOS\", \"EOS\"]\n",
    "USE_CHORDS = False\n",
    "USE_RESTS = False\n",
    "USE_TEMPOS = True\n",
    "USE_TIME_SIGNATURE = False\n",
    "USE_PROGRAMS = False\n",
    "NUM_TEMPOS = 32\n",
    "TEMPO_RANGE = (50, 200)  # (min_tempo, max_tempo)\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": PITCH_RANGE,\n",
    "    \"beat_res\": BEAT_RES,\n",
    "    \"num_velocities\": NUM_VELOCITIES,\n",
    "    \"special_tokens\": SPECIAL_TOKENS,\n",
    "    \"use_chords\": USE_CHORDS,\n",
    "    \"use_rests\": USE_RESTS,\n",
    "    \"use_tempos\": USE_TEMPOS,\n",
    "    \"use_time_signatures\": USE_TIME_SIGNATURE,\n",
    "    \"use_programs\": USE_PROGRAMS,\n",
    "    \"num_tempos\": NUM_TEMPOS,\n",
    "    \"tempo_range\": TEMPO_RANGE,\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer\n",
    "# tokenizer = REMI(config)\n",
    "tokenizer = REMI(params='./tokenizer.json')\n",
    "\n",
    "# Trains the tokenizer with Byte Pair Encoding (BPE) to build the vocabulary, here 10k tokens\n",
    "midi_paths = list(Path('../data/maestro-v3.0.0').glob('**/*.mid')) + list(Path('../data/maestro-v3.0.0').glob('**/*.midi'))\n",
    "# tokenizer.learn_bpe(\n",
    "#     vocab_size=10000,\n",
    "#     files_paths=midi_paths,\n",
    "#     start_from_empty_voc=False,\n",
    "# )\n",
    "# tokenizer.save_params(\"tokenizer.json\")\n",
    "\n",
    "# Split MIDI paths in train/valid/test sets\n",
    "total_num_files = len(midi_paths)\n",
    "num_files_valid = round(total_num_files * 0.2)\n",
    "num_files_test = round(total_num_files * 0.1)\n",
    "shuffle(midi_paths)\n",
    "midi_paths_valid = midi_paths[:num_files_valid]\n",
    "midi_paths_test = midi_paths[num_files_valid:num_files_valid + num_files_test]\n",
    "midi_paths_train = midi_paths[num_files_valid + num_files_test:]\n",
    "\n",
    "# Loads tokens and create data collator\n",
    "kwargs_dataset = {\"min_seq_len\": 256, \"max_seq_len\": 1024, \"tokenizer\": tokenizer}\n",
    "dataset_train = DatasetTok(midi_paths_train, **kwargs_dataset)\n",
    "dataset_valid = DatasetTok(midi_paths_valid, **kwargs_dataset)\n",
    "dataset_test = DatasetTok(midi_paths_test, **kwargs_dataset)\n",
    "collator = DataCollator(\n",
    "    tokenizer[\"PAD_None\"], tokenizer[\"BOS_None\"], tokenizer[\"EOS_None\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = MambaConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=192,\n",
    "    state_size=8,\n",
    "    max_position_embeddings=8192,\n",
    "    num_hidden_layers=16,\n",
    "    pad_token_id=tokenizer['PAD_None'],\n",
    "    bos_token_id=tokenizer['BOS_None'],\n",
    "    eos_token_id=tokenizer['EOS_None'],\n",
    ")\n",
    "\n",
    "model = MambaForCausalLM(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 16\n",
      "***** Running training *****\n",
      "  Num examples = 7,677\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 3\n",
      "  Total optimization steps = 300\n",
      "  Number of trainable parameters = 5,800,128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8208cc85cde245c688b6c7c8c8d3e834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filif/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.7185, 'grad_norm': 1.0116009712219238, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.12}\n",
      "{'loss': 10.3971, 'grad_norm': 0.9841909408569336, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.25}\n",
      "{'loss': 9.7775, 'grad_norm': 0.8079961538314819, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.38}\n",
      "{'loss': 9.1375, 'grad_norm': 0.5027562975883484, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.5}\n",
      "{'loss': 8.7133, 'grad_norm': 0.39267244935035706, 'learning_rate': 9.944154131125642e-05, 'epoch': 0.62}\n",
      "{'loss': 8.3953, 'grad_norm': 0.33455485105514526, 'learning_rate': 9.504844339512095e-05, 'epoch': 0.75}\n",
      "{'loss': 8.1963, 'grad_norm': 0.29263848066329956, 'learning_rate': 8.665259359149132e-05, 'epoch': 0.88}\n",
      "{'loss': 8.0577, 'grad_norm': 0.25291773676872253, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.0}\n",
      "{'loss': 7.9943, 'grad_norm': 0.20593200623989105, 'learning_rate': 6.112604669781572e-05, 'epoch': 1.12}\n",
      "{'loss': 7.937, 'grad_norm': 0.19787374138832092, 'learning_rate': 4.626349532067879e-05, 'epoch': 1.25}\n",
      "{'loss': 7.9036, 'grad_norm': 0.21083061397075653, 'learning_rate': 3.173294878168025e-05, 'epoch': 1.38}\n",
      "{'loss': 7.8887, 'grad_norm': 0.2003031075000763, 'learning_rate': 1.8825509907063327e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to runs/checkpoint-250\n",
      "Configuration saved in runs/checkpoint-250/config.json\n",
      "Configuration saved in runs/checkpoint-250/generation_config.json\n",
      "Model weights saved in runs/checkpoint-250/model.safetensors\n",
      "/home/filif/miniconda3/envs/pixgan/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.877, 'grad_norm': 0.1827237755060196, 'learning_rate': 8.688061284200266e-06, 'epoch': 1.62}\n",
      "{'loss': 7.8688, 'grad_norm': 0.19901618361473083, 'learning_rate': 2.221359710692961e-06, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to runs\n",
      "Configuration saved in runs/config.json\n",
      "Configuration saved in runs/generation_config.json\n",
      "Model weights saved in runs/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.878, 'grad_norm': 0.19314920902252197, 'learning_rate': 0.0, 'epoch': 1.88}\n",
      "{'train_runtime': 261.2715, 'train_samples_per_second': 55.115, 'train_steps_per_second': 1.148, 'train_loss': 8.582714284261067, 'epoch': 1.88}\n",
      "***** train metrics *****\n",
      "  epoch                    =       1.88\n",
      "  train_loss               =     8.5827\n",
      "  train_runtime            = 0:04:21.27\n",
      "  train_samples_per_second =     55.115\n",
      "  train_steps_per_second   =      1.148\n"
     ]
    }
   ],
   "source": [
    "def preprocess_logits(logits: Tensor, _: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess the logits before accumulating them during evaluation.\n",
    "\n",
    "    This allows to significantly reduce the memory usage and make the training tractable.\n",
    "    \"\"\"\n",
    "    pred_ids = argmax(logits, dim=-1)  # long dtype\n",
    "    return pred_ids\n",
    "\n",
    "# Create config for the Trainer\n",
    "USE_CUDA = cuda_available()\n",
    "if not cuda_available():\n",
    "    FP16 = FP16_EVAL = BF16 = BF16_EVAL = False\n",
    "elif is_bf16_supported():\n",
    "    BF16 = BF16_EVAL = True\n",
    "    FP16 = FP16_EVAL = False\n",
    "else:\n",
    "    BF16 = BF16_EVAL = False\n",
    "    FP16 = FP16_EVAL = True\n",
    "USE_MPS = not USE_CUDA and mps_available()\n",
    "training_config = TrainingArguments(\n",
    "    \"runs\", False, True, False, False, \"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=48,\n",
    "    gradient_accumulation_steps=3,\n",
    "    eval_accumulation_steps=None,\n",
    "    eval_steps=1000,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=3.0,\n",
    "    max_steps=300,\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    warmup_ratio=0.3,\n",
    "    log_level=\"debug\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=5,\n",
    "    no_cuda=not USE_CUDA,\n",
    "    seed=444,\n",
    "    fp16=FP16,\n",
    "    fp16_full_eval=FP16_EVAL,\n",
    "    bf16=BF16,\n",
    "    bf16_full_eval=BF16_EVAL,\n",
    "    # load_best_model_at_end=True,\n",
    "    label_smoothing_factor=0.,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=[\"tensorboard\"],\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "collator = DataCollator(tokenizer[\"PAD_None\"], tokenizer[\"BOS_None\"], tokenizer[\"EOS_None\"], copy_inputs_as_labels=True)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_config,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_valid,\n",
    "    callbacks=None,\n",
    "    preprocess_logits_for_metrics=preprocess_logits,\n",
    ")\n",
    "\n",
    "# Training\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too\n",
    "trainer.log_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_metrics(\"train\", train_result.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./runs/checkpoint-250/config.json\n",
      "Model config MambaConfig {\n",
      "  \"architectures\": [\n",
      "    \"MambaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 2,\n",
      "  \"conv_kernel\": 4,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"expand\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 192,\n",
      "  \"initializer_range\": 0.1,\n",
      "  \"intermediate_size\": 384,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"mamba\",\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"rescale_prenorm_residual\": false,\n",
      "  \"residual_in_fp32\": true,\n",
      "  \"state_size\": 8,\n",
      "  \"time_step_floor\": 0.0001,\n",
      "  \"time_step_init_scheme\": \"random\",\n",
      "  \"time_step_max\": 0.1,\n",
      "  \"time_step_min\": 0.001,\n",
      "  \"time_step_rank\": 12,\n",
      "  \"time_step_scale\": 1.0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_bias\": false,\n",
      "  \"use_cache\": true,\n",
      "  \"use_conv_bias\": true,\n",
      "  \"vocab_size\": 10000\n",
      "}\n",
      "\n",
      "loading weights file ./runs/checkpoint-250/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MambaForCausalLM.\n",
      "\n",
      "All the weights of MambaForCausalLM were initialized from the model checkpoint at ./runs/checkpoint-250.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MambaForCausalLM for predictions without further training.\n",
      "loading configuration file ./runs/checkpoint-250/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MambaForCausalLM.from_pretrained('./runs/checkpoint-250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model.generate(max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokens_to_midi(out).dump_midi('krowakrowa.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
